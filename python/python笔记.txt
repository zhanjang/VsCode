文件读取和写入
基本格式:
变量 = open("文件路径","打开模式",encoding="编码方式")

打开模式:r模式(只读) w模式(写入(原有内容删除)) a模式(追加(写在文件后面,原有内容不删除))

r模式的方法
read(字节数) 读取指定字节的内容,默认全部
readlines() 读取每一行内容生成一个列表
reaaline() 只读取一行内容
close() 关闭文件解除占用 可以通过with open("文件路径","打开模式",encoding="编码方式") as 变量的方法在结束文件调用后自动关闭文件

w模式的方法 若文件不存在会自动创建
write(内容) 向缓冲区中写入内容 注:调用close或flush方法后才会真正进入文件
writelines() 向缓冲区写入列表内的字符串,只能是字符串不能是其他类型数据 注:不会换行要自行输入\n,\t等字符
flush()吧缓冲区文件写入文件
close()和r方法一样但可以执行flush的功能

a模式的方法和w模式一样

python操作mysql
1.connection方法需要手动传入 host地址,port端口,user用户名,password密码  获取连接对象
2.操作数据库方法
	1.先用cursor方法获取游标对象
	2.select_db('数据库名称')选择数据库    和sql中的use功能一样
	3.执行sql前要调用游标对象来执行命令,命令写法同sql例如 游标对象名称.execute("sql语言命令")
	4.查询结果通过游标对象.fetchall()来获取   返回数据为元组
	5.改动数据的行为需要代码手动确认   连接对象.commit()来确认更改   注:可以通过第一步手动为autocommit参数传入True来自动确认更改


python json文件基础操作  引入json包 dumps函数把python容器编译为json字符串  loads把json字符串编译成python容器


pyspark库操作
1.SparkConf().setMaster("local[*]")运行终端 本地或分布式服务器.setAppName("test_spark")项目名称
2.sc = SparkContext(conf=conf)获取运行环境入口 只能通过这种方式
3.SparkContest方法
	1.parallelize将数据转为rdd对象传入运行终端中
	2.collect查看rdd中的内容
	3.stop停止rdd程序
	4.主义给pyspark传入pyton解释器的位置